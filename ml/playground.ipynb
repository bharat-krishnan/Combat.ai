{"cells":[{"cell_type":"markdown","metadata":{"id":"pd-u8YII3Mm7"},"source":["# Imports\n","\n","we want to import openCV, mediapipe, numpy"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"XWYlOQPR3MnA"},"outputs":[],"source":["import cv2\n","import mediapipe as mp\n","import numpy"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TMF4roTI3MnC"},"outputs":[],"source":["mp_holistic = mp.solutions.holistic # Holistic model\n","mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n","\n","def mediapipe_detection(image, model):\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n","    image.flags.writeable = False                  # Image is no longer writeable\n","    results = model.process(image)                 # Make prediction\n","    image.flags.writeable = True                   # Image is now writeable \n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n","    return image, results\n","\n","def draw_landmarks(image, results):\n","    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n","  \n","def draw_styled_landmarks(image, results):\n","\n","    # Draw pose connections\n","    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n","                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n","                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n","                             ) \n"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"ppyGiyuM3MnD","outputId":"0c233329-e9f6-4548-deb2-ca59f84a5dc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["20 - 0\n","19 - 1\n","18 - 2\n","17 - 3\n","16 - 4\n","15 - 5\n","14 - 6\n","13 - 7\n","12 - 8\n","11 - 9\n","10 - 10\n","9 - 11\n","8 - 12\n","7 - 13\n","6 - 14\n","5 - 15\n","4 - 16\n","3 - 17\n","2 - 18\n","1 - 19\n","0 - 20\n","0 - 120\n","0 - 220\n","0 - 320\n","20 - 320\n","19 - 321\n","18 - 322\n","17 - 323\n","16 - 324\n","15 - 325\n","14 - 326\n","13 - 327\n","12 - 328\n","11 - 329\n","10 - 330\n","9 - 331\n","8 - 332\n","7 - 333\n","6 - 334\n","5 - 335\n","4 - 336\n","3 - 337\n","2 - 338\n","1 - 339\n","0 - 340\n","0 - 339\n","0 - 338\n","0 - 337\n","0 - 336\n","0 - 335\n","0 - 334\n","0 - 333\n","0 - 332\n","0 - 331\n","0 - 330\n","0 - 331\n","0 - 330\n","0 - 329\n","0 - 330\n","0 - 331\n","0 - 332\n","0 - 333\n","0 - 334\n","0 - 335\n","0 - 336\n","0 - 335\n","0 - 336\n","0 - 337\n","0 - 338\n","0 - 337\n","20 - 337\n","19 - 338\n","18 - 339\n","17 - 340\n","16 - 341\n","15 - 342\n","14 - 343\n","13 - 344\n","12 - 345\n","11 - 346\n","10 - 347\n","9 - 348\n","8 - 349\n","7 - 350\n","6 - 351\n","5 - 352\n","4 - 353\n","3 - 354\n","2 - 355\n","1 - 356\n","0 - 357\n","0 - 356\n","0 - 355\n","0 - 356\n","0 - 357\n","0 - 358\n","0 - 359\n","0 - 360\n"]}],"source":["cap = cv2.VideoCapture('./videos/Cross.MOV')\n","frames = []\n","frameCount = 0\n","jumpFrame = 31\n","\n","# Set mediapipe model \n","\n","with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","    while cap.isOpened():\n","        if jumpFrame > 0:\n","            jumpFrame -= 1\n","        print(str(jumpFrame) + \" - \" + str(frameCount))\n","\n","        ret, frame = cap.read()\n","\n","        # Make detections\n","        image, results = mediapipe_detection(frame, holistic)\n","\n","        frame = []\n","        if results.pose_landmarks:\n","            for res in results.pose_landmarks.landmark:\n","                if res.visibility > .65:\n","                    frame.append(res.x)\n","                    frame.append(res.y)\n","                    frame.append(res.z)\n","                else:\n","                    frame.append(0)\n","                    frame.append(0)\n","                    frame.append(0)\n","\n","            frames.append(frame)\n","            # Draw landmarks\n","            draw_styled_landmarks(image, results)\n","\n","            # Show to screen\n","\n","        cv2.putText(image, text= str(frameCount), org=(500, 100),\n","                fontFace= cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(0,0,0),\n","                thickness=2, lineType=cv2.LINE_AA)\n","\n","        cv2.imshow('OpenCV Feed', image)\n","        \n","        wait = 10 if jumpFrame > 0 else 0\n","        \n","        key = cv2.waitKey(wait)\n","        if jumpFrame == 0:\n","            if key == ord('w'):\n","                frameCount += 100\n","                cap.set(cv2.CAP_PROP_POS_FRAMES, frameCount)\n","                continue\n","            if key == ord('p'):\n","                frameCount -= 1\n","                cap.set(cv2.CAP_PROP_POS_FRAMES, frameCount)\n","                continue\n","            if key == ord('s'):\n","                jumpFrame = 21\n","                continue\n","        if key == ord('q'):\n","            break\n","            # Quit when 'q' is pressed\n","\n","        \n","        frameCount += 1\n","\n","        # if jumpFrame == 0:\n","        #     key = cv2.waitKey(0)\n","            \n","        #     while key not in [ord('q'), ord('k'), ord('p'), ord('c')]:\n","        #         key = cv2.waitKey(0)\n","\n","        #     if cv2.waitKey(0) & 0xFF == ord('c'):\n","        #         jumpFrame = 21\n","     \n","        \n","\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","\n","# print(frames)"]}],"metadata":{"colab":{"name":"playground.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.7.1 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"21cc3dc4b5603a15ce10e0f2a275a0772e29eb093bef1b46e9e8aa7d14ea3b3d"}}},"nbformat":4,"nbformat_minor":0}
